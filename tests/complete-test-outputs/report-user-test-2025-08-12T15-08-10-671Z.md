# n8n Workflow Builder Test Report

**Test Name:** User Test
**Timestamp:** 2025-08-12T15:06:20.181Z
**Duration:** 110490ms
**Success:** ✅ Yes

## User Prompt
```
Create a research agent which uses apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword which I put in. Then, output a all the links to the top performing posts and YouTube videos into a Google sheets
```

## Session ID: `complete_e2e_user_test_1755011180180`

## ========== DISCOVERY PHASE ==========

**Status:** ✅ Success
**Duration:** 18103ms
**Memory Delta:** 2MB

### Logs
```
2025-08-12T15:06:20.604Z [INFO] [Orchestrator] Starting discovery phase
2025-08-12T15:06:20.604Z [DEBUG] [Claude] Sending request for discovery phase
2025-08-12T15:06:32.669Z [INFO] [MCP] Calling tool: get_node_for_task
2025-08-12T15:06:32.669Z [DEBUG] [MCP] Parameters: {"task":"receive_webhook"}
2025-08-12T15:06:32.670Z [INFO] [MCP] Calling tool: get_node_for_task
2025-08-12T15:06:32.670Z [DEBUG] [MCP] Parameters: {"task":"multi_tool_ai_agent"}
2025-08-12T15:06:32.670Z [INFO] [MCP] Calling tool: get_node_for_task
2025-08-12T15:06:32.670Z [DEBUG] [MCP] Parameters: {"task":"filter_data"}
2025-08-12T15:06:32.670Z [INFO] [MCP] Calling tool: get_node_for_task
2025-08-12T15:06:32.671Z [DEBUG] [MCP] Parameters: {"task":"transform_data"}
2025-08-12T15:06:32.671Z [INFO] [MCP] Calling tool: get_node_for_task
2025-08-12T15:06:32.671Z [DEBUG] [MCP] Parameters: {"task":"use_google_sheets_as_tool"}
2025-08-12T15:06:32.828Z [DEBUG] [MCP] Tool get_node_for_task completed successfully
2025-08-12T15:06:32.830Z [DEBUG] [MCP] Tool get_node_for_task completed successfully
2025-08-12T15:06:32.867Z [DEBUG] [MCP] Tool get_node_for_task completed successfully
2025-08-12T15:06:32.913Z [DEBUG] [MCP] Tool get_node_for_task completed successfully
2025-08-12T15:06:32.923Z [DEBUG] [MCP] Tool get_node_for_task completed successfully
2025-08-12T15:06:32.925Z [INFO] [MCP] Calling tool: search_nodes
2025-08-12T15:06:32.925Z [DEBUG] [MCP] Parameters: {"query":"apify","limit":20}
2025-08-12T15:06:32.925Z [INFO] [MCP] Calling tool: search_nodes
2025-08-12T15:06:32.925Z [DEBUG] [MCP] Parameters: {"query":"apify","limit":20}
2025-08-12T15:06:33.042Z [DEBUG] [MCP] Tool search_nodes completed successfully
2025-08-12T15:06:33.042Z [INFO] [MCP] Calling tool: search_nodes
2025-08-12T15:06:33.042Z [DEBUG] [MCP] Parameters: {"query":"linkedin","limit":20}
2025-08-12T15:06:33.053Z [DEBUG] [MCP] Tool search_nodes completed successfully
2025-08-12T15:06:33.053Z [INFO] [MCP] Calling tool: search_nodes
2025-08-12T15:06:33.053Z [DEBUG] [MCP] Parameters: {"query":"youtube","limit":20}
2025-08-12T15:06:33.161Z [DEBUG] [MCP] Tool search_nodes completed successfully
2025-08-12T15:06:33.172Z [DEBUG] [MCP] Tool search_nodes completed successfully
2025-08-12T15:06:38.650Z [INFO] [Tools] Added node: nodes-base.webhook (Set up a webhook to receive data from external services)
2025-08-12T15:06:38.650Z [INFO] [Tools] Added node: nodes-base.googleSheets (Use Google Sheets as an AI tool for reading/writing data)
2025-08-12T15:06:38.650Z [INFO] [Tools] Added node: nodes-base.if (Filter items based on conditions)
2025-08-12T15:06:38.650Z [INFO] [Tools] Added node: nodes-base.code (Transform data structure using JavaScript)
2025-08-12T15:06:38.650Z [INFO] [Tools] Added node: nodes-langchain.agent (AI agent with multiple tools for complex automation)
2025-08-12T15:06:38.650Z [INFO] [Tools] Added node: nodes-base.linkedIn (Access LinkedIn API to scrape top performing posts based on keywords)
2025-08-12T15:06:38.650Z [INFO] [Tools] Added node: nodes-base.youTube (Access YouTube API to scrape top performing videos based on keywords)
2025-08-12T15:06:38.707Z [DEBUG] [Orchestrator] Session state updated
2025-08-12T15:06:38.707Z [INFO] [Orchestrator] Data flow captured: 3 transformations
```

### Nodes
- **nodes-base.webhook** (ID: task_node_1)
  - Purpose: Set up a webhook to receive data from external services
- **nodes-base.googleSheets** (ID: task_node_5)
  - Purpose: Use Google Sheets as an AI tool for reading/writing data
- **nodes-base.if** (ID: task_node_3)
  - Purpose: Filter items based on conditions
- **nodes-base.code** (ID: task_node_4)
  - Purpose: Transform data structure using JavaScript
- **nodes-langchain.agent** (ID: task_node_2)
  - Purpose: AI agent with multiple tools for complex automation
- **nodes-base.linkedIn** (ID: search_node_1)
  - Purpose: Access LinkedIn API to scrape top performing posts based on keywords
- **nodes-base.youTube** (ID: search_node_2)
  - Purpose: Access YouTube API to scrape top performing videos based on keywords

### Data Flow
**Input:**
```json
{
  "prompt": "Create a research agent which uses apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword which I put in. Then, output a all the links to the top performing posts and YouTube videos into a Google sheets"
}
```

**Output:**
```json
{
  "nodes": [
    {
      "id": "task_node_1",
      "type": "nodes-base.webhook",
      "displayName": "receive webhook",
      "purpose": "Set up a webhook to receive data from external services",
      "isPreConfigured": true,
      "config": {
        "httpMethod": "POST",
        "path": "webhook",
        "responseMode": "lastNode",
        "responseData": "allEntries",
        "onError": "continueRegularOutput",
        "alwaysOutputData": true
      }
    },
    {
      "id": "task_node_5",
      "type": "nodes-base.googleSheets",
      "displayName": "use google sheets as tool",
      "purpose": "Use Google Sheets as an AI tool for reading/writing data",
      "isPreConfigured": true,
      "config": {
        "operation": "append",
        "sheetId": "={{ $fromAI(\"sheetId\", \"The Google Sheets ID\") }}",
        "range": "={{ $fromAI(\"range\", \"The range to append to, e.g. A:Z\") }}",
        "dataMode": "autoMap"
      }
    },
    {
      "id": "task_node_3",
      "type": "nodes-base.if",
      "displayName": "filter data",
      "purpose": "Filter items based on conditions",
      "isPreConfigured": true,
      "config": {
        "conditions": {
          "conditions": [
            {
              "leftValue": "",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ]
        }
      }
    },
    {
      "id": "task_node_4",
      "type": "nodes-base.code",
      "displayName": "transform data",
      "purpose": "Transform data structure using JavaScript",
      "isPreConfigured": true,
      "config": {
        "language": "javaScript",
        "jsCode": "// Transform each item\nconst results = [];\n\nfor (const item of items) {\n  results.push({\n    json: {\n      // Transform your data here\n      id: item.json.id,\n      processedAt: new Date().toISOString()\n    }\n  });\n}\n\nreturn results;"
      }
    },
    {
      "id": "task_node_2",
      "type": "nodes-langchain.agent",
      "displayName": "multi tool ai agent",
      "purpose": "AI agent with multiple tools for complex automation",
      "isPreConfigured": true,
      "config": {
        "text": "={{ $json.query }}",
        "outputType": "output",
        "systemMessage": "You are an intelligent assistant with access to multiple tools. Use them wisely to complete tasks."
      }
    },
    {
      "id": "search_node_1",
      "type": "nodes-base.linkedIn",
      "displayName": "nodes-base.linkedIn",
      "purpose": "Access LinkedIn API to scrape top performing posts based on keywords",
      "needsConfiguration": true
    },
    {
      "id": "search_node_2",
      "type": "nodes-base.youTube",
      "displayName": "nodes-base.youTube",
      "purpose": "Access YouTube API to scrape top performing videos based on keywords",
      "needsConfiguration": true
    }
  ]
}
```

**Transformations:**
- Intent Analysis
- Node Discovery
- Selection

### Session State (Baseline)
```json
{
  "sessionId": "complete_e2e_user_test_1755011180180",
  "createdAt": "2025-08-12T15:06:20.586Z",
  "state": {
    "phase": "discovery",
    "userPrompt": "Create a research agent which uses apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword which I put in. Then, output a all the links to the top performing posts and YouTube videos into a Google sheets",
    "discovered": [
      {
        "id": "task_node_1",
        "type": "nodes-base.webhook",
        "config": {
          "path": "webhook",
          "onError": "continueRegularOutput",
          "httpMethod": "POST",
          "responseData": "allEntries",
          "responseMode": "lastNode",
          "alwaysOutputData": true
        },
        "purpose": "Set up a webhook to receive data from external services",
        "displayName": "receive webhook",
        "isPreConfigured": true
      },
      {
        "id": "task_node_5",
        "type": "nodes-base.googleSheets",
        "config": {
          "range": "={{ $fromAI(\"range\", \"The range to append to, e.g. A:Z\") }}",
          "sheetId": "={{ $fromAI(\"sheetId\", \"The Google Sheets ID\") }}",
          "dataMode": "autoMap",
          "operation": "append"
        },
        "purpose": "Use Google Sheets as an AI tool for reading/writing data",
        "displayName": "use google sheets as tool",
        "isPreConfigured": true
      },
      {
        "id": "task_node_3",
        "type": "nodes-base.if",
        "config": {
          "conditions": {
            "conditions": [
              {
                "operator": {
                  "type": "string",
                  "operation": "equals"
                },
                "leftValue": "",
                "rightValue": ""
              }
            ]
          }
        },
        "purpose": "Filter items based on conditions",
        "displayName": "filter data",
        "isPreConfigured": true
      },
      {
        "id": "task_node_4",
        "type": "nodes-base.code",
        "config": {
          "jsCode": "// Transform each item\nconst results = [];\n\nfor (const item of items) {\n  results.push({\n    json: {\n      // Transform your data here\n      id: item.json.id,\n      processedAt: new Date().toISOString()\n    }\n  });\n}\n\nreturn results;",
          "language": "javaScript"
        },
        "purpose": "Transform data structure using JavaScript",
        "displayName": "transform data",
        "isPreConfigured": true
      },
      {
        "id": "task_node_2",
        "type": "nodes-langchain.agent",
        "config": {
          "text": "={{ $json.query }}",
          "outputType": "output",
          "systemMessage": "You are an intelligent assistant with access to multiple tools. Use them wisely to complete tasks."
        },
        "purpose": "AI agent with multiple tools for complex automation",
        "displayName": "multi tool ai agent",
        "isPreConfigured": true
      },
      {
        "id": "search_node_1",
        "type": "nodes-base.linkedIn",
        "purpose": "Access LinkedIn API to scrape top performing posts based on keywords"
      },
      {
        "id": "search_node_2",
        "type": "nodes-base.youTube",
        "purpose": "Access YouTube API to scrape top performing videos based on keywords"
      }
    ],
    "selected": [
      "task_node_1",
      "task_node_5",
      "task_node_3",
      "task_node_4",
      "task_node_2",
      "search_node_1",
      "search_node_2"
    ],
    "configured": {},
    "validated": {},
    "workflow": {
      "nodes": [],
      "settings": {},
      "connections": {}
    },
    "buildPhases": [],
    "operationHistory": [
      {
        "node": {
          "id": "task_node_1",
          "type": "nodes-base.webhook",
          "config": {
            "path": "webhook",
            "onError": "continueRegularOutput",
            "httpMethod": "POST",
            "responseData": "allEntries",
            "responseMode": "lastNode",
            "alwaysOutputData": true
          },
          "purpose": "Set up a webhook to receive data from external services",
          "displayName": "receive webhook",
          "isPreConfigured": true
        },
        "type": "discoverNode",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "type": "selectNode",
        "nodeId": "task_node_1",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "node": {
          "id": "task_node_5",
          "type": "nodes-base.googleSheets",
          "config": {
            "range": "={{ $fromAI(\"range\", \"The range to append to, e.g. A:Z\") }}",
            "sheetId": "={{ $fromAI(\"sheetId\", \"The Google Sheets ID\") }}",
            "dataMode": "autoMap",
            "operation": "append"
          },
          "purpose": "Use Google Sheets as an AI tool for reading/writing data",
          "displayName": "use google sheets as tool",
          "isPreConfigured": true
        },
        "type": "discoverNode",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "type": "selectNode",
        "nodeId": "task_node_5",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "node": {
          "id": "task_node_3",
          "type": "nodes-base.if",
          "config": {
            "conditions": {
              "conditions": [
                {
                  "operator": {
                    "type": "string",
                    "operation": "equals"
                  },
                  "leftValue": "",
                  "rightValue": ""
                }
              ]
            }
          },
          "purpose": "Filter items based on conditions",
          "displayName": "filter data",
          "isPreConfigured": true
        },
        "type": "discoverNode",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "type": "selectNode",
        "nodeId": "task_node_3",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "node": {
          "id": "task_node_4",
          "type": "nodes-base.code",
          "config": {
            "jsCode": "// Transform each item\nconst results = [];\n\nfor (const item of items) {\n  results.push({\n    json: {\n      // Transform your data here\n      id: item.json.id,\n      processedAt: new Date().toISOString()\n    }\n  });\n}\n\nreturn results;",
            "language": "javaScript"
          },
          "purpose": "Transform data structure using JavaScript",
          "displayName": "transform data",
          "isPreConfigured": true
        },
        "type": "discoverNode",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "type": "selectNode",
        "nodeId": "task_node_4",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "node": {
          "id": "task_node_2",
          "type": "nodes-langchain.agent",
          "config": {
            "text": "={{ $json.query }}",
            "outputType": "output",
            "systemMessage": "You are an intelligent assistant with access to multiple tools. Use them wisely to complete tasks."
          },
          "purpose": "AI agent with multiple tools for complex automation",
          "displayName": "multi tool ai agent",
          "isPreConfigured": true
        },
        "type": "discoverNode",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "type": "selectNode",
        "nodeId": "task_node_2",
        "timestamp": "2025-08-12T15:06:38.245Z"
      },
      {
        "node": {
          "id": "search_node_1",
          "type": "nodes-base.linkedIn",
          "purpose": "Access LinkedIn API to scrape top performing posts based on keywords"
        },
        "type": "discoverNode",
        "timestamp": "2025-08-12T15:06:38.440Z"
      },
      {
        "type": "selectNode",
        "nodeId": "search_node_1",
        "timestamp": "2025-08-12T15:06:38.440Z"
      },
      {
        "node": {
          "id": "search_node_2",
          "type": "nodes-base.youTube",
          "purpose": "Access YouTube API to scrape top performing videos based on keywords"
        },
        "type": "discoverNode",
        "timestamp": "2025-08-12T15:06:38.440Z"
      },
      {
        "type": "selectNode",
        "nodeId": "search_node_2",
        "timestamp": "2025-08-12T15:06:38.440Z"
      }
    ],
    "pendingClarifications": [],
    "clarificationHistory": []
  }
}
```

## ========== CONFIGURATION PHASE ==========

**Status:** ✅ Success
**Duration:** 46618ms
**Memory Delta:** 5MB

### Logs
```
2025-08-12T15:06:38.707Z [INFO] [Orchestrator] Starting configuration phase
2025-08-12T15:06:38.707Z [DEBUG] [Claude] Sending request for configuration phase
2025-08-12T15:06:38.799Z [INFO] [MCP] Calling tool: get_node_essentials
2025-08-12T15:06:38.799Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.webhook"}
2025-08-12T15:06:38.799Z [INFO] [MCP] Calling tool: get_node_essentials
2025-08-12T15:06:38.799Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.googleSheets"}
2025-08-12T15:06:38.799Z [INFO] [MCP] Calling tool: get_node_essentials
2025-08-12T15:06:38.799Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.if"}
2025-08-12T15:06:38.799Z [INFO] [MCP] Calling tool: get_node_essentials
2025-08-12T15:06:38.799Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.code"}
2025-08-12T15:06:38.799Z [INFO] [MCP] Calling tool: get_node_essentials
2025-08-12T15:06:38.799Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-langchain.agent"}
2025-08-12T15:06:38.799Z [INFO] [MCP] Calling tool: get_node_essentials
2025-08-12T15:06:38.799Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.linkedIn"}
2025-08-12T15:06:38.799Z [INFO] [MCP] Calling tool: get_node_essentials
2025-08-12T15:06:38.799Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube"}
2025-08-12T15:06:38.966Z [DEBUG] [MCP] Tool get_node_essentials completed successfully
2025-08-12T15:06:38.969Z [DEBUG] [MCP] Tool get_node_essentials completed successfully
2025-08-12T15:06:38.984Z [DEBUG] [MCP] Tool get_node_essentials completed successfully
2025-08-12T15:06:39.023Z [DEBUG] [MCP] Tool get_node_essentials completed successfully
2025-08-12T15:06:39.046Z [DEBUG] [MCP] Tool get_node_essentials completed successfully
2025-08-12T15:06:39.173Z [DEBUG] [MCP] Tool get_node_essentials completed successfully
2025-08-12T15:06:39.440Z [DEBUG] [MCP] Tool get_node_essentials completed successfully
2025-08-12T15:06:45.687Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:06:45.687Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.if","config":{"conditions":{"conditions":[{"operator":{"type":"string","operation":"contains"},"leftValue":"{{ $json.url }}","rightValue":"linkedin.com"}]}}}
2025-08-12T15:06:45.807Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:06:45.807Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.webhook","config":{"path":"research-agent-trigger","httpMethod":"POST","responseData":"allEntries","responseMode":"lastNode"}}
2025-08-12T15:06:45.866Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:06:45.978Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:06:47.257Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:06:47.257Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.googleSheets","config":{"range":"A:C","sheetId":"={{ $fromAI(\"sheetId\", \"The Google Sheets ID where scraped content will be stored\") }}","dataMode":"defineBelow","operation":"append","columns":{"mappingMode":"defineBelow","value":{"Platform":"={{ $json.platform }}","Title":"={{ $json.title }}","URL":"={{ $json.url }}"}}}}
2025-08-12T15:06:47.373Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:06:48.722Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:06:48.722Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-langchain.agent","config":{"text":"{{ $json.keyword }}","outputType":"output","systemMessage":"You are a research agent specialized in finding top performing content on social media platforms. Your task is to:\n\n1. Use Apify tools to scrape LinkedIn posts based on the provided keyword\n2. Use Apify tools to scrape YouTube videos based on the provided keyword\n3. Identify the top performing content based on engagement metrics (likes, comments, shares, views)\n4. Extract the direct links to these posts and videos\n5. Organize the results in a structured format suitable for Google Sheets output\n6. Return a comprehensive list of links to the top performing LinkedIn posts and YouTube videos\n\nAlways prioritize content with high engagement rates and recent publication dates. Focus on extracting clean, direct URLs that can be easily accessed."}}
2025-08-12T15:06:48.891Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:06:48.935Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:06:48.935Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"search","maxResults":20}
2025-08-12T15:06:49.041Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:06:51.910Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:06:51.910Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"order","maxResults":20}
2025-08-12T15:06:52.026Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:06:52.641Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:06:52.641Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.linkedIn","config":{"resource":"post","postAs":"person","person":"","binaryPropertyName":"data"}}
2025-08-12T15:06:52.760Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:06:54.237Z [INFO] [MCP] Calling tool: get_node_documentation
2025-08-12T15:06:54.237Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube"}
2025-08-12T15:06:54.343Z [DEBUG] [MCP] Tool get_node_documentation completed successfully
2025-08-12T15:06:56.148Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:06:56.148Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.code","config":{"jsCode":"// Transform scraped data from Apify for Google Sheets output\nconst results = [];\n\nfor (const item of $input.all()) {\n  const data = item.json;\n  \n  // Handle LinkedIn posts\n  if (data.platform === 'linkedin' || data.type === 'linkedin_post') {\n    results.push({\n      json: {\n        platform: 'LinkedIn',\n        title: data.text || data.content || data.description || 'No title',\n        link: data.url || data.postUrl || data.link,\n        engagement: data.likes || data.reactions || 0,\n        comments: data.comments || 0,\n        shares: data.shares || data.reposts || 0,\n        author: data.author || data.authorName || 'Unknown',\n        publishedDate: data.publishedAt || data.createdAt || data.date || new Date().toISOString(),\n        keyword: data.searchKeyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n  \n  // Handle YouTube videos\n  if (data.platform === 'youtube' || data.type === 'youtube_video') {\n    results.push({\n      json: {\n        platform: 'YouTube',\n        title: data.title || data.name || 'No title',\n        link: data.url || data.videoUrl || `https://youtube.com/watch?v=${data.videoId}`,\n        engagement: data.views || data.viewCount || 0,\n        comments: data.comments || data.commentCount || 0,\n        shares: data.likes || data.likeCount || 0,\n        author: data.channelName || data.author || data.uploader || 'Unknown',\n        publishedDate: data.publishedAt || data.uploadDate || data.date || new Date().toISOString(),\n        keyword: data.searchKeyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n  \n  // Handle generic scraped data (fallback)\n  if (!data.platform && !data.type) {\n    results.push({\n      json: {\n        platform: data.source || 'Unknown',\n        title: data.title || data.text || data.content || 'No title',\n        link: data.url || data.link || 'No link',\n        engagement: data.engagement || data.likes || data.views || 0,\n        comments: data.comments || 0,\n        shares: data.shares || data.likes || 0,\n        author: data.author || data.creator || 'Unknown',\n        publishedDate: data.date || data.publishedAt || new Date().toISOString(),\n        keyword: data.keyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n}\n\nreturn results;","language":"javaScript"}}
2025-08-12T15:06:56.259Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:06:56.699Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:06:56.699Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"getAll","maxResults":20}
2025-08-12T15:06:56.836Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:06:58.871Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:06:58.871Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"video","maxResults":20}
2025-08-12T15:06:58.979Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:07:01.299Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:07:01.299Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"operation","maxResults":20}
2025-08-12T15:07:01.426Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:07:04.594Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:07:04.594Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"chart","maxResults":20}
2025-08-12T15:07:04.712Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:07:07.789Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:07:07.789Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"part","maxResults":20}
2025-08-12T15:07:07.937Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:07:10.920Z [INFO] [MCP] Calling tool: search_node_properties
2025-08-12T15:07:10.920Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","query":"filters","maxResults":20}
2025-08-12T15:07:11.037Z [DEBUG] [MCP] Tool search_node_properties completed successfully
2025-08-12T15:07:16.269Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:07:16.270Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","config":{"notes":"Searches YouTube for top performing videos based on keyword input","typeVersion":1,"parameters":{"resource":"video","operation":"getAll","part":["snippet","statistics"],"filters":{"q":"{{ $json.keyword }}","order":"viewCount"},"options":{"maxResults":50}}}}
2025-08-12T15:07:16.456Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:07:23.103Z [INFO] [MCP] Calling tool: validate_node_minimal
2025-08-12T15:07:23.103Z [DEBUG] [MCP] Parameters: {"nodeType":"nodes-base.youTube","config":{"resource":"video","operation":"getAll","part":["snippet","statistics"],"filters":{"q":"{{ $json.keyword }}"},"options":{"order":"viewCount","maxResults":50}}}
2025-08-12T15:07:23.254Z [DEBUG] [MCP] Tool validate_node_minimal completed successfully
2025-08-12T15:07:25.212Z [INFO] [Tools] Added node: nodes-base.webhook (Set up a webhook to receive data from external services)
2025-08-12T15:07:25.212Z [INFO] [Tools] Added node: nodes-base.googleSheets (Use Google Sheets as an AI tool for reading/writing data)
2025-08-12T15:07:25.212Z [INFO] [Tools] Added node: nodes-base.if (Filter items based on conditions)
2025-08-12T15:07:25.212Z [INFO] [Tools] Added node: nodes-base.code (Transform data structure using JavaScript)
2025-08-12T15:07:25.212Z [INFO] [Tools] Added node: nodes-langchain.agent (AI agent with multiple tools for complex automation)
2025-08-12T15:07:25.212Z [INFO] [Tools] Added node: nodes-base.linkedIn (Access LinkedIn API to scrape top performing posts based on keywords)
2025-08-12T15:07:25.212Z [INFO] [Tools] Added node: nodes-base.youTube (Access YouTube API to scrape top performing videos based on keywords)
2025-08-12T15:07:25.325Z [DEBUG] [Orchestrator] Session state updated
2025-08-12T15:07:25.325Z [INFO] [Orchestrator] Data flow captured: 3 transformations
```

### Nodes
- **nodes-base.webhook** (ID: task_node_1)
  - Purpose: Set up a webhook to receive data from external services
  - Validation: valid
- **nodes-base.googleSheets** (ID: task_node_5)
  - Purpose: Use Google Sheets as an AI tool for reading/writing data
  - Validation: valid
- **nodes-base.if** (ID: task_node_3)
  - Purpose: Filter items based on conditions
  - Validation: valid
- **nodes-base.code** (ID: task_node_4)
  - Purpose: Transform data structure using JavaScript
  - Validation: valid
- **nodes-langchain.agent** (ID: task_node_2)
  - Purpose: AI agent with multiple tools for complex automation
  - Validation: valid
- **nodes-base.linkedIn** (ID: search_node_1)
  - Purpose: Access LinkedIn API to scrape top performing posts based on keywords
  - Validation: valid
- **nodes-base.youTube** (ID: search_node_2)
  - Purpose: Access YouTube API to scrape top performing videos based on keywords
  - Validation: valid

### Data Flow
**Input:**
```json
{
  "discoveredNodes": [
    {
      "id": "task_node_1",
      "type": "nodes-base.webhook",
      "displayName": "receive webhook",
      "purpose": "Set up a webhook to receive data from external services",
      "isPreConfigured": true,
      "config": {
        "httpMethod": "POST",
        "path": "webhook",
        "responseMode": "lastNode",
        "responseData": "allEntries",
        "onError": "continueRegularOutput",
        "alwaysOutputData": true
      }
    },
    {
      "id": "task_node_5",
      "type": "nodes-base.googleSheets",
      "displayName": "use google sheets as tool",
      "purpose": "Use Google Sheets as an AI tool for reading/writing data",
      "isPreConfigured": true,
      "config": {
        "operation": "append",
        "sheetId": "={{ $fromAI(\"sheetId\", \"The Google Sheets ID\") }}",
        "range": "={{ $fromAI(\"range\", \"The range to append to, e.g. A:Z\") }}",
        "dataMode": "autoMap"
      }
    },
    {
      "id": "task_node_3",
      "type": "nodes-base.if",
      "displayName": "filter data",
      "purpose": "Filter items based on conditions",
      "isPreConfigured": true,
      "config": {
        "conditions": {
          "conditions": [
            {
              "leftValue": "",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ]
        }
      }
    },
    {
      "id": "task_node_4",
      "type": "nodes-base.code",
      "displayName": "transform data",
      "purpose": "Transform data structure using JavaScript",
      "isPreConfigured": true,
      "config": {
        "language": "javaScript",
        "jsCode": "// Transform each item\nconst results = [];\n\nfor (const item of items) {\n  results.push({\n    json: {\n      // Transform your data here\n      id: item.json.id,\n      processedAt: new Date().toISOString()\n    }\n  });\n}\n\nreturn results;"
      }
    },
    {
      "id": "task_node_2",
      "type": "nodes-langchain.agent",
      "displayName": "multi tool ai agent",
      "purpose": "AI agent with multiple tools for complex automation",
      "isPreConfigured": true,
      "config": {
        "text": "={{ $json.query }}",
        "outputType": "output",
        "systemMessage": "You are an intelligent assistant with access to multiple tools. Use them wisely to complete tasks."
      }
    },
    {
      "id": "search_node_1",
      "type": "nodes-base.linkedIn",
      "displayName": "nodes-base.linkedIn",
      "purpose": "Access LinkedIn API to scrape top performing posts based on keywords",
      "needsConfiguration": true
    },
    {
      "id": "search_node_2",
      "type": "nodes-base.youTube",
      "displayName": "nodes-base.youTube",
      "purpose": "Access YouTube API to scrape top performing videos based on keywords",
      "needsConfiguration": true
    }
  ]
}
```

**Output:**
```json
{
  "configuredNodes": [
    {
      "id": "task_node_1",
      "type": "nodes-base.webhook",
      "purpose": "Set up a webhook to receive data from external services",
      "config": {
        "onError": "continueRegularOutput",
        "alwaysOutputData": true,
        "notes": "Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input",
        "typeVersion": "1",
        "parameters": {
          "path": "research-agent-trigger",
          "httpMethod": "POST",
          "responseData": "allEntries",
          "responseMode": "lastNode"
        }
      },
      "validated": true
    },
    {
      "id": "task_node_5",
      "type": "nodes-base.googleSheets",
      "purpose": "Use Google Sheets as an AI tool for reading/writing data",
      "config": {
        "notes": "Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis",
        "typeVersion": "1",
        "parameters": {
          "range": "A:C",
          "sheetId": "={{ $fromAI(\"sheetId\", \"The Google Sheets ID where scraped content will be stored\") }}",
          "dataMode": "defineBelow",
          "operation": "append",
          "columns": {
            "mappingMode": "defineBelow",
            "value": {
              "Platform": "={{ $json.platform }}",
              "Title": "={{ $json.title }}",
              "URL": "={{ $json.url }}"
            }
          }
        }
      },
      "validated": true
    },
    {
      "id": "task_node_3",
      "type": "nodes-base.if",
      "purpose": "Filter items based on conditions",
      "config": {
        "notes": "Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets",
        "typeVersion": "1",
        "parameters": {
          "conditions": {
            "conditions": [
              {
                "operator": {
                  "type": "string",
                  "operation": "contains"
                },
                "leftValue": "{{ $json.url }}",
                "rightValue": "linkedin.com"
              }
            ]
          }
        }
      },
      "validated": true
    },
    {
      "id": "task_node_4",
      "type": "nodes-base.code",
      "purpose": "Transform data structure using JavaScript",
      "config": {
        "notes": "Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics",
        "parameters": {
          "jsCode": "// Transform scraped data from Apify for Google Sheets output\nconst results = [];\n\nfor (const item of $input.all()) {\n  const data = item.json;\n  \n  // Handle LinkedIn posts\n  if (data.platform === 'linkedin' || data.type === 'linkedin_post') {\n    results.push({\n      json: {\n        platform: 'LinkedIn',\n        title: data.text || data.content || data.description || 'No title',\n        link: data.url || data.postUrl || data.link,\n        engagement: data.likes || data.reactions || 0,\n        comments: data.comments || 0,\n        shares: data.shares || data.reposts || 0,\n        author: data.author || data.authorName || 'Unknown',\n        publishedDate: data.publishedAt || data.createdAt || data.date || new Date().toISOString(),\n        keyword: data.searchKeyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n  \n  // Handle YouTube videos\n  if (data.platform === 'youtube' || data.type === 'youtube_video') {\n    results.push({\n      json: {\n        platform: 'YouTube',\n        title: data.title || data.name || 'No title',\n        link: data.url || data.videoUrl || `https://youtube.com/watch?v=${data.videoId}`,\n        engagement: data.views || data.viewCount || 0,\n        comments: data.comments || data.commentCount || 0,\n        shares: data.likes || data.likeCount || 0,\n        author: data.channelName || data.author || data.uploader || 'Unknown',\n        publishedDate: data.publishedAt || data.uploadDate || data.date || new Date().toISOString(),\n        keyword: data.searchKeyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n  \n  // Handle generic scraped data (fallback)\n  if (!data.platform && !data.type) {\n    results.push({\n      json: {\n        platform: data.source || 'Unknown',\n        title: data.title || data.text || data.content || 'No title',\n        link: data.url || data.link || 'No link',\n        engagement: data.engagement || data.likes || data.views || 0,\n        comments: data.comments || 0,\n        shares: data.shares || data.likes || 0,\n        author: data.author || data.creator || 'Unknown',\n        publishedDate: data.date || data.publishedAt || new Date().toISOString(),\n        keyword: data.keyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n}\n\nreturn results;",
          "language": "javaScript"
        }
      },
      "validated": true
    },
    {
      "id": "task_node_2",
      "type": "nodes-langchain.agent",
      "purpose": "AI agent with multiple tools for complex automation",
      "config": {
        "notes": "Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets",
        "typeVersion": "1",
        "parameters": {
          "text": "{{ $json.keyword }}",
          "outputType": "output",
          "systemMessage": "You are a research agent specialized in finding top performing content on social media platforms. Your task is to:\n\n1. Use Apify tools to scrape LinkedIn posts based on the provided keyword\n2. Use Apify tools to scrape YouTube videos based on the provided keyword\n3. Identify the top performing content based on engagement metrics (likes, comments, shares, views)\n4. Extract the direct links to these posts and videos\n5. Organize the results in a structured format suitable for Google Sheets output\n6. Return a comprehensive list of links to the top performing LinkedIn posts and YouTube videos\n\nAlways prioritize content with high engagement rates and recent publication dates. Focus on extracting clean, direct URLs that can be easily accessed."
        }
      },
      "validated": true
    },
    {
      "id": "search_node_1",
      "type": "nodes-base.linkedIn",
      "purpose": "Access LinkedIn API to scrape top performing posts based on keywords",
      "config": {
        "notes": "This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead",
        "typeVersion": "1",
        "parameters": {
          "resource": "post",
          "postAs": "person",
          "person": "",
          "binaryPropertyName": "data"
        }
      },
      "validated": true
    },
    {
      "id": "search_node_2",
      "type": "nodes-base.youTube",
      "purpose": "Access YouTube API to scrape top performing videos based on keywords",
      "config": {
        "notes": "Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics",
        "typeVersion": 1,
        "parameters": {
          "resource": "video",
          "operation": "getAll",
          "part": [
            "snippet",
            "statistics"
          ],
          "filters": {
            "q": "{{ $json.keyword }}"
          },
          "options": {
            "order": "viewCount",
            "maxResults": 50
          }
        }
      },
      "validated": true
    }
  ]
}
```

**Transformations:**
- Parameter Configuration
- Validation
- Type Checking

### Session State Changes
**Changed from discovery phase:**

**Modified:**
- `state`: {11 fields} → {11 fields}

**Unchanged (2 fields):** sessionId, createdAt

## ========== BUILDING PHASE ==========

**Status:** ✅ Success
**Duration:** 26046ms
**Memory Delta:** -3MB

### Logs
```
2025-08-12T15:07:25.326Z [INFO] [Orchestrator] Starting building phase
2025-08-12T15:07:25.326Z [DEBUG] [Claude] Sending request for building phase
2025-08-12T15:07:51.234Z [INFO] [Tools] Added node: n8n-nodes-base.webhook (Webhook Trigger)
2025-08-12T15:07:51.234Z [INFO] [Tools] Added node: n8n-nodes-langchain.agent (Research Agent)
2025-08-12T15:07:51.234Z [INFO] [Tools] Added node: n8n-nodes-base.code (Transform Data)
2025-08-12T15:07:51.234Z [INFO] [Tools] Added node: n8n-nodes-base.if (Filter Content)
2025-08-12T15:07:51.234Z [INFO] [Tools] Added node: n8n-nodes-base.googleSheets (Save to Google Sheets)
2025-08-12T15:07:51.234Z [INFO] [Tools] Added node: n8n-nodes-base.youTube (YouTube Search)
2025-08-12T15:07:51.234Z [INFO] [Tools] Added node: n8n-nodes-base.linkedIn (LinkedIn Posts)
2025-08-12T15:07:51.234Z [INFO] [Orchestrator] Created 7 nodes
2025-08-12T15:07:51.234Z [INFO] [Orchestrator] Created 4 connection groups
2025-08-12T15:07:51.372Z [DEBUG] [Orchestrator] Session state updated
2025-08-12T15:07:51.372Z [INFO] [Orchestrator] Data flow captured: 3 transformations
```

### Nodes
- **n8n-nodes-base.webhook** (ID: webhook_1)
  - Purpose: Webhook Trigger
- **n8n-nodes-langchain.agent** (ID: agent_1)
  - Purpose: Research Agent
- **n8n-nodes-base.code** (ID: code_1)
  - Purpose: Transform Data
- **n8n-nodes-base.if** (ID: if_1)
  - Purpose: Filter Content
- **n8n-nodes-base.googleSheets** (ID: googleSheets_1)
  - Purpose: Save to Google Sheets
- **n8n-nodes-base.youTube** (ID: youTube_1)
  - Purpose: YouTube Search
- **n8n-nodes-base.linkedIn** (ID: linkedIn_1)
  - Purpose: LinkedIn Posts

### Data Flow
**Input:**
```json
{
  "configuredNodes": [
    {
      "id": "task_node_1",
      "type": "nodes-base.webhook",
      "purpose": "Set up a webhook to receive data from external services",
      "config": {
        "onError": "continueRegularOutput",
        "alwaysOutputData": true,
        "notes": "Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input",
        "typeVersion": "1",
        "parameters": {
          "path": "research-agent-trigger",
          "httpMethod": "POST",
          "responseData": "allEntries",
          "responseMode": "lastNode"
        }
      },
      "validated": true
    },
    {
      "id": "task_node_5",
      "type": "nodes-base.googleSheets",
      "purpose": "Use Google Sheets as an AI tool for reading/writing data",
      "config": {
        "notes": "Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis",
        "typeVersion": "1",
        "parameters": {
          "range": "A:C",
          "sheetId": "={{ $fromAI(\"sheetId\", \"The Google Sheets ID where scraped content will be stored\") }}",
          "dataMode": "defineBelow",
          "operation": "append",
          "columns": {
            "mappingMode": "defineBelow",
            "value": {
              "Platform": "={{ $json.platform }}",
              "Title": "={{ $json.title }}",
              "URL": "={{ $json.url }}"
            }
          }
        }
      },
      "validated": true
    },
    {
      "id": "task_node_3",
      "type": "nodes-base.if",
      "purpose": "Filter items based on conditions",
      "config": {
        "notes": "Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets",
        "typeVersion": "1",
        "parameters": {
          "conditions": {
            "conditions": [
              {
                "operator": {
                  "type": "string",
                  "operation": "contains"
                },
                "leftValue": "{{ $json.url }}",
                "rightValue": "linkedin.com"
              }
            ]
          }
        }
      },
      "validated": true
    },
    {
      "id": "task_node_4",
      "type": "nodes-base.code",
      "purpose": "Transform data structure using JavaScript",
      "config": {
        "notes": "Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics",
        "parameters": {
          "jsCode": "// Transform scraped data from Apify for Google Sheets output\nconst results = [];\n\nfor (const item of $input.all()) {\n  const data = item.json;\n  \n  // Handle LinkedIn posts\n  if (data.platform === 'linkedin' || data.type === 'linkedin_post') {\n    results.push({\n      json: {\n        platform: 'LinkedIn',\n        title: data.text || data.content || data.description || 'No title',\n        link: data.url || data.postUrl || data.link,\n        engagement: data.likes || data.reactions || 0,\n        comments: data.comments || 0,\n        shares: data.shares || data.reposts || 0,\n        author: data.author || data.authorName || 'Unknown',\n        publishedDate: data.publishedAt || data.createdAt || data.date || new Date().toISOString(),\n        keyword: data.searchKeyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n  \n  // Handle YouTube videos\n  if (data.platform === 'youtube' || data.type === 'youtube_video') {\n    results.push({\n      json: {\n        platform: 'YouTube',\n        title: data.title || data.name || 'No title',\n        link: data.url || data.videoUrl || `https://youtube.com/watch?v=${data.videoId}`,\n        engagement: data.views || data.viewCount || 0,\n        comments: data.comments || data.commentCount || 0,\n        shares: data.likes || data.likeCount || 0,\n        author: data.channelName || data.author || data.uploader || 'Unknown',\n        publishedDate: data.publishedAt || data.uploadDate || data.date || new Date().toISOString(),\n        keyword: data.searchKeyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n  \n  // Handle generic scraped data (fallback)\n  if (!data.platform && !data.type) {\n    results.push({\n      json: {\n        platform: data.source || 'Unknown',\n        title: data.title || data.text || data.content || 'No title',\n        link: data.url || data.link || 'No link',\n        engagement: data.engagement || data.likes || data.views || 0,\n        comments: data.comments || 0,\n        shares: data.shares || data.likes || 0,\n        author: data.author || data.creator || 'Unknown',\n        publishedDate: data.date || data.publishedAt || new Date().toISOString(),\n        keyword: data.keyword || 'N/A',\n        processedAt: new Date().toISOString()\n      }\n    });\n  }\n}\n\nreturn results;",
          "language": "javaScript"
        }
      },
      "validated": true
    },
    {
      "id": "task_node_2",
      "type": "nodes-langchain.agent",
      "purpose": "AI agent with multiple tools for complex automation",
      "config": {
        "notes": "Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets",
        "typeVersion": "1",
        "parameters": {
          "text": "{{ $json.keyword }}",
          "outputType": "output",
          "systemMessage": "You are a research agent specialized in finding top performing content on social media platforms. Your task is to:\n\n1. Use Apify tools to scrape LinkedIn posts based on the provided keyword\n2. Use Apify tools to scrape YouTube videos based on the provided keyword\n3. Identify the top performing content based on engagement metrics (likes, comments, shares, views)\n4. Extract the direct links to these posts and videos\n5. Organize the results in a structured format suitable for Google Sheets output\n6. Return a comprehensive list of links to the top performing LinkedIn posts and YouTube videos\n\nAlways prioritize content with high engagement rates and recent publication dates. Focus on extracting clean, direct URLs that can be easily accessed."
        }
      },
      "validated": true
    },
    {
      "id": "search_node_1",
      "type": "nodes-base.linkedIn",
      "purpose": "Access LinkedIn API to scrape top performing posts based on keywords",
      "config": {
        "notes": "This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead",
        "typeVersion": "1",
        "parameters": {
          "resource": "post",
          "postAs": "person",
          "person": "",
          "binaryPropertyName": "data"
        }
      },
      "validated": true
    },
    {
      "id": "search_node_2",
      "type": "nodes-base.youTube",
      "purpose": "Access YouTube API to scrape top performing videos based on keywords",
      "config": {
        "notes": "Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics",
        "typeVersion": 1,
        "parameters": {
          "resource": "video",
          "operation": "getAll",
          "part": [
            "snippet",
            "statistics"
          ],
          "filters": {
            "q": "{{ $json.keyword }}"
          },
          "options": {
            "order": "viewCount",
            "maxResults": 50
          }
        }
      },
      "validated": true
    }
  ]
}
```

**Output:**
```json
{
  "workflow": {
    "name": "Research Agent - LinkedIn & YouTube Scraper to Google Sheets",
    "nodes": [
      {
        "id": "webhook_1",
        "type": "n8n-nodes-base.webhook",
        "position": [
          250,
          300
        ],
        "parameters": {
          "notes": "Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"
        },
        "name": "Webhook Trigger",
        "typeVersion": 1,
        "onError": "stopWorkflow"
      },
      {
        "id": "agent_1",
        "type": "n8n-nodes-langchain.agent",
        "position": [
          550,
          300
        ],
        "parameters": {
          "notes": "Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets",
          "text": "{{ $json.keyword }}"
        },
        "name": "Research Agent",
        "typeVersion": 1,
        "onError": "continueRegularOutput"
      },
      {
        "id": "code_1",
        "type": "n8n-nodes-base.code",
        "position": [
          850,
          300
        ],
        "parameters": {
          "notes": "Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics",
          "jsCode": "// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"
        },
        "name": "Transform Data",
        "typeVersion": 2,
        "onError": "continueRegularOutput"
      },
      {
        "id": "if_1",
        "type": "n8n-nodes-base.if",
        "position": [
          1150,
          300
        ],
        "parameters": {
          "notes": "Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets",
          "conditions": {
            "conditions": [
              {
                "leftValue": "{{ $json.platform }}",
                "rightValue": "LinkedIn",
                "operator": {
                  "type": "string",
                  "operation": "equals"
                }
              }
            ]
          }
        },
        "name": "Filter Content",
        "typeVersion": 2,
        "onError": "continueRegularOutput"
      },
      {
        "id": "googleSheets_1",
        "type": "n8n-nodes-base.googleSheets",
        "position": [
          1450,
          300
        ],
        "parameters": {
          "notes": "Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis",
          "operation": "append",
          "range": "A:E",
          "columns": {
            "value": {
              "Platform": "={{ $json.platform }}",
              "URL": "={{ $json.url }}",
              "Title": "={{ $json.title }}",
              "Engagement": "={{ $json.engagement }}",
              "Type": "={{ $json.type }}"
            }
          }
        },
        "name": "Save to Google Sheets",
        "typeVersion": 4,
        "onError": "continueErrorOutput"
      },
      {
        "id": "youTube_1",
        "type": "n8n-nodes-base.youTube",
        "position": [
          550,
          500
        ],
        "parameters": {
          "notes": "Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics",
          "operation": "search",
          "part": [
            "snippet",
            "statistics"
          ],
          "q": "={{ $json.keyword }}",
          "order": "viewCount",
          "maxResults": 10
        },
        "name": "YouTube Search",
        "typeVersion": 2,
        "onError": "continueErrorOutput"
      },
      {
        "id": "linkedIn_1",
        "type": "n8n-nodes-base.linkedIn",
        "position": [
          550,
          100
        ],
        "parameters": {
          "notes": "This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead",
          "operation": "post",
          "person": ""
        },
        "name": "LinkedIn Posts",
        "typeVersion": 1,
        "onError": "continueErrorOutput"
      }
    ],
    "connections": {
      "Webhook Trigger": {
        "main": [
          [
            {
              "node": "Research Agent",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Research Agent": {
        "main": [
          [
            {
              "node": "Transform Data",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Transform Data": {
        "main": [
          [
            {
              "node": "Filter Content",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Filter Content": {
        "main": [
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ],
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    },
    "settings": {
      "executionOrder": "v1",
      "saveDataSuccessExecution": "all",
      "saveDataErrorExecution": "all",
      "saveManualExecutions": true
    }
  }
}
```

**Transformations:**
- Workflow Generation
- Connection Building
- Settings Configuration

### Session State Changes
**Changed from configuration phase:**

**Added:**
- `state`: {1 fields}

**Modified:**
- `state`: {11 fields} → {11 fields}

**Unchanged (2 fields):** sessionId, createdAt

## ========== VALIDATION PHASE ==========

**Status:** ✅ Success
**Duration:** 18628ms
**Memory Delta:** 1MB

### Logs
```
2025-08-12T15:07:51.373Z [INFO] [Orchestrator] Starting validation phase
2025-08-12T15:07:51.373Z [DEBUG] [Claude] Sending request for validation phase
2025-08-12T15:07:51.374Z [INFO] [MCP] Calling tool: validate_workflow
2025-08-12T15:07:51.374Z [DEBUG] [MCP] Parameters: {"workflow":{"name":"Research Agent - LinkedIn & YouTube Scraper to Google Sheets","nodes":[{"id":"webhook_1","type":"n8n-nodes-base.webhook","position":[250,300],"parameters":{"notes":"Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"},"name":"Webhook Trigger","typeVersion":1,"onError":"stopWorkflow"},{"id":"agent_1","type":"n8n-nodes-langchain.agent","position":[550,300],"parameters":{"notes":"Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets","text":"{{ $json.keyword }}"},"name":"Research Agent","typeVersion":1,"onError":"continueRegularOutput"},{"id":"code_1","type":"n8n-nodes-base.code","position":[850,300],"parameters":{"notes":"Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics","jsCode":"// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"},"name":"Transform Data","typeVersion":2,"onError":"continueRegularOutput"},{"id":"if_1","type":"n8n-nodes-base.if","position":[1150,300],"parameters":{"notes":"Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets","conditions":{"conditions":[{"leftValue":"{{ $json.platform }}","rightValue":"LinkedIn","operator":{"type":"string","operation":"equals"}}]}},"name":"Filter Content","typeVersion":2,"onError":"continueRegularOutput"},{"id":"googleSheets_1","type":"n8n-nodes-base.googleSheets","position":[1450,300],"parameters":{"notes":"Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis","operation":"append","range":"A:E","columns":{"value":{"Platform":"={{ $json.platform }}","URL":"={{ $json.url }}","Title":"={{ $json.title }}","Engagement":"={{ $json.engagement }}","Type":"={{ $json.type }}"}}},"name":"Save to Google Sheets","typeVersion":4,"onError":"continueErrorOutput"},{"id":"youTube_1","type":"n8n-nodes-base.youTube","position":[550,500],"parameters":{"notes":"Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics","operation":"search","part":["snippet","statistics"],"q":"={{ $json.keyword }}","order":"viewCount","maxResults":10},"name":"YouTube Search","typeVersion":2,"onError":"continueErrorOutput"},{"id":"linkedIn_1","type":"n8n-nodes-base.linkedIn","position":[550,100],"parameters":{"notes":"This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead","operation":"post","person":""},"name":"LinkedIn Posts","typeVersion":1,"onError":"continueErrorOutput"}],"connections":{"Webhook Trigger":{"main":[[{"node":"Research Agent","type":"main","index":0}]]},"Research Agent":{"main":[[{"node":"Transform Data","type":"main","index":0}]]},"Transform Data":{"main":[[{"node":"Filter Content","type":"main","index":0}]]},"Filter Content":{"main":[[{"node":"Save to Google Sheets","type":"main","index":0}],[{"node":"Save to Google Sheets","type":"main","index":0}]]}},"settings":{"executionOrder":"v1","saveDataSuccessExecution":"all","saveDataErrorExecution":"all","saveManualExecutions":true}},"options":{"validateNodes":true,"validateConnections":true,"validateExpressions":true,"profile":"runtime"}}
2025-08-12T15:07:51.544Z [DEBUG] [MCP] Tool validate_workflow completed successfully
2025-08-12T15:08:06.602Z [INFO] [MCP] Calling tool: validate_workflow
2025-08-12T15:08:06.602Z [DEBUG] [MCP] Parameters: {"workflow":{"name":"Research Agent - LinkedIn & YouTube Scraper to Google Sheets","nodes":[{"id":"webhook_1","type":"n8n-nodes-base.webhook","position":[250,300],"parameters":{"path":"research-agent","httpMethod":"POST","responseMode":"onReceived"},"name":"Webhook Trigger","typeVersion":2,"onError":"stopWorkflow","notes":"Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"},{"id":"agent_1","type":"@n8n/n8n-nodes-langchain.agent","position":[550,300],"parameters":{"text":"{{ $json.keyword }}"},"name":"Research Agent","typeVersion":1,"onError":"continueRegularOutput","notes":"Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets"},{"id":"code_1","type":"n8n-nodes-base.code","position":[850,300],"parameters":{"jsCode":"// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"},"name":"Transform Data","typeVersion":2,"onError":"continueRegularOutput","notes":"Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics"},{"id":"if_1","type":"n8n-nodes-base.if","position":[1150,300],"parameters":{"conditions":{"conditions":[{"leftValue":"{{ $json.platform }}","rightValue":"LinkedIn","operator":{"type":"string","operation":"equals"}}]}},"name":"Filter Content","typeVersion":2.2,"onError":"continueRegularOutput","notes":"Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets"},{"id":"googleSheets_1","type":"n8n-nodes-base.googleSheets","position":[1450,300],"parameters":{"operation":"append","range":"A:E","columns":{"value":{"Platform":"={{ $json.platform }}","URL":"={{ $json.url }}","Title":"={{ $json.title }}","Engagement":"={{ $json.engagement }}","Type":"={{ $json.type }}"}}},"name":"Save to Google Sheets","typeVersion":4.6,"onError":"continueErrorOutput","notes":"Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis"},{"id":"youTube_1","type":"n8n-nodes-base.youTube","position":[550,500],"parameters":{"operation":"search","part":["snippet","statistics"],"q":"={{ $json.keyword }}","order":"viewCount","maxResults":10},"name":"YouTube Search","typeVersion":2,"onError":"continueErrorOutput","notes":"Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics"},{"id":"linkedIn_1","type":"n8n-nodes-base.linkedIn","position":[550,100],"parameters":{"operation":"post","person":""},"name":"LinkedIn Posts","typeVersion":1,"onError":"continueErrorOutput","notes":"This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead"}],"connections":{"Webhook Trigger":{"main":[[{"node":"Research Agent","type":"main","index":0}]]},"Research Agent":{"main":[[{"node":"Transform Data","type":"main","index":0}]]},"Transform Data":{"main":[[{"node":"Filter Content","type":"main","index":0}]]},"Filter Content":{"main":[[{"node":"Save to Google Sheets","type":"main","index":0}],[{"node":"Save to Google Sheets","type":"main","index":0}]]}},"settings":{"executionOrder":"v1","saveDataSuccessExecution":"all","saveDataErrorExecution":"all","saveManualExecutions":true}},"options":{"validateNodes":true,"validateConnections":true,"validateExpressions":true,"profile":"runtime"}}
2025-08-12T15:08:06.819Z [DEBUG] [MCP] Tool validate_workflow completed successfully
2025-08-12T15:08:09.015Z [INFO] [MCP] Calling tool: validate_workflow
2025-08-12T15:08:09.015Z [DEBUG] [MCP] Parameters: {"workflow":{"name":"Research Agent - LinkedIn & YouTube Scraper to Google Sheets","nodes":[{"id":"webhook_1","type":"n8n-nodes-base.webhook","position":[250,300],"parameters":{"path":"research-agent","httpMethod":"POST","responseMode":"onReceived"},"name":"Webhook Trigger","typeVersion":2,"onError":"stopWorkflow","notes":"Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"},{"id":"agent_1","type":"@n8n/n8n-nodes-langchain.agent","position":[550,300],"parameters":{"text":"{{ $json.keyword }}"},"name":"Research Agent","typeVersion":2,"onError":"continueRegularOutput","notes":"Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets"},{"id":"code_1","type":"n8n-nodes-base.code","position":[850,300],"parameters":{"jsCode":"// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"},"name":"Transform Data","typeVersion":2,"onError":"continueRegularOutput","notes":"Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics"},{"id":"if_1","type":"n8n-nodes-base.if","position":[1150,300],"parameters":{"conditions":{"conditions":[{"leftValue":"{{ $json.platform }}","rightValue":"LinkedIn","operator":{"type":"string","operation":"equals"}}]}},"name":"Filter Content","typeVersion":2.2,"onError":"continueRegularOutput","notes":"Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets"},{"id":"googleSheets_1","type":"n8n-nodes-base.googleSheets","position":[1450,300],"parameters":{"operation":"append","range":"A:E","columns":{"value":{"Platform":"={{ $json.platform }}","URL":"={{ $json.url }}","Title":"={{ $json.title }}","Engagement":"={{ $json.engagement }}","Type":"={{ $json.type }}"}}},"name":"Save to Google Sheets","typeVersion":4.6,"onError":"continueErrorOutput","notes":"Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis"},{"id":"youTube_1","type":"n8n-nodes-base.youTube","position":[550,500],"parameters":{"operation":"search","part":["snippet","statistics"],"q":"={{ $json.keyword }}","order":"viewCount","maxResults":10},"name":"YouTube Search","typeVersion":2,"onError":"continueErrorOutput","notes":"Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics"},{"id":"linkedIn_1","type":"n8n-nodes-base.linkedIn","position":[550,100],"parameters":{"operation":"post","person":""},"name":"LinkedIn Posts","typeVersion":1,"onError":"continueErrorOutput","notes":"This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead"}],"connections":{"Webhook Trigger":{"main":[[{"node":"Research Agent","type":"main","index":0}]]},"Research Agent":{"main":[[{"node":"Transform Data","type":"main","index":0}]]},"Transform Data":{"main":[[{"node":"Filter Content","type":"main","index":0}]]},"Filter Content":{"main":[[{"node":"Save to Google Sheets","type":"main","index":0}],[{"node":"Save to Google Sheets","type":"main","index":0}]]}},"settings":{"executionOrder":"v1","saveDataSuccessExecution":"all","saveDataErrorExecution":"all","saveManualExecutions":true}},"options":{"validateNodes":true,"validateConnections":true,"validateExpressions":true,"profile":"runtime"}}
2025-08-12T15:08:09.190Z [DEBUG] [MCP] Tool validate_workflow completed successfully
2025-08-12T15:08:09.907Z [INFO] [Validation] Applied fix 1: Replaced 7 nodes (attempt 1)
2025-08-12T15:08:09.907Z [INFO] [Validation] Applied fix 2: Replaced 1 nodes (attempt 2)
2025-08-12T15:08:09.907Z [INFO] [Orchestrator] Validation completed in 3 attempts
2025-08-12T15:08:09.907Z [INFO] [Tools] Applied 2 fixes
2025-08-12T15:08:09.998Z [DEBUG] [Orchestrator] Session state updated
2025-08-12T15:08:10.000Z [INFO] [Orchestrator] Data flow captured: 3 transformations
```

### Data Flow
**Input:**
```json
{
  "workflow": {
    "name": "Research Agent - LinkedIn & YouTube Scraper to Google Sheets",
    "nodes": [
      {
        "id": "webhook_1",
        "type": "n8n-nodes-base.webhook",
        "position": [
          250,
          300
        ],
        "parameters": {
          "notes": "Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"
        },
        "name": "Webhook Trigger",
        "typeVersion": 1,
        "onError": "stopWorkflow"
      },
      {
        "id": "agent_1",
        "type": "n8n-nodes-langchain.agent",
        "position": [
          550,
          300
        ],
        "parameters": {
          "notes": "Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets",
          "text": "{{ $json.keyword }}"
        },
        "name": "Research Agent",
        "typeVersion": 1,
        "onError": "continueRegularOutput"
      },
      {
        "id": "code_1",
        "type": "n8n-nodes-base.code",
        "position": [
          850,
          300
        ],
        "parameters": {
          "notes": "Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics",
          "jsCode": "// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"
        },
        "name": "Transform Data",
        "typeVersion": 2,
        "onError": "continueRegularOutput"
      },
      {
        "id": "if_1",
        "type": "n8n-nodes-base.if",
        "position": [
          1150,
          300
        ],
        "parameters": {
          "notes": "Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets",
          "conditions": {
            "conditions": [
              {
                "leftValue": "{{ $json.platform }}",
                "rightValue": "LinkedIn",
                "operator": {
                  "type": "string",
                  "operation": "equals"
                }
              }
            ]
          }
        },
        "name": "Filter Content",
        "typeVersion": 2,
        "onError": "continueRegularOutput"
      },
      {
        "id": "googleSheets_1",
        "type": "n8n-nodes-base.googleSheets",
        "position": [
          1450,
          300
        ],
        "parameters": {
          "notes": "Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis",
          "operation": "append",
          "range": "A:E",
          "columns": {
            "value": {
              "Platform": "={{ $json.platform }}",
              "URL": "={{ $json.url }}",
              "Title": "={{ $json.title }}",
              "Engagement": "={{ $json.engagement }}",
              "Type": "={{ $json.type }}"
            }
          }
        },
        "name": "Save to Google Sheets",
        "typeVersion": 4,
        "onError": "continueErrorOutput"
      },
      {
        "id": "youTube_1",
        "type": "n8n-nodes-base.youTube",
        "position": [
          550,
          500
        ],
        "parameters": {
          "notes": "Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics",
          "operation": "search",
          "part": [
            "snippet",
            "statistics"
          ],
          "q": "={{ $json.keyword }}",
          "order": "viewCount",
          "maxResults": 10
        },
        "name": "YouTube Search",
        "typeVersion": 2,
        "onError": "continueErrorOutput"
      },
      {
        "id": "linkedIn_1",
        "type": "n8n-nodes-base.linkedIn",
        "position": [
          550,
          100
        ],
        "parameters": {
          "notes": "This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead",
          "operation": "post",
          "person": ""
        },
        "name": "LinkedIn Posts",
        "typeVersion": 1,
        "onError": "continueErrorOutput"
      }
    ],
    "connections": {
      "Webhook Trigger": {
        "main": [
          [
            {
              "node": "Research Agent",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Research Agent": {
        "main": [
          [
            {
              "node": "Transform Data",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Transform Data": {
        "main": [
          [
            {
              "node": "Filter Content",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Filter Content": {
        "main": [
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ],
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    },
    "settings": {
      "executionOrder": "v1",
      "saveDataSuccessExecution": "all",
      "saveDataErrorExecution": "all",
      "saveManualExecutions": true
    }
  }
}
```

**Output:**
```json
{
  "validatedWorkflow": {
    "name": "Research Agent - LinkedIn & YouTube Scraper to Google Sheets",
    "nodes": [
      {
        "id": "webhook_1",
        "type": "n8n-nodes-base.webhook",
        "position": [
          250,
          300
        ],
        "parameters": {
          "path": "research-agent",
          "httpMethod": "POST",
          "responseMode": "onReceived"
        },
        "name": "Webhook Trigger",
        "typeVersion": 2,
        "onError": "stopWorkflow",
        "notes": "Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"
      },
      {
        "id": "agent_1",
        "type": "@n8n/n8n-nodes-langchain.agent",
        "position": [
          550,
          300
        ],
        "parameters": {
          "text": "{{ $json.keyword }}"
        },
        "name": "Research Agent",
        "typeVersion": 2,
        "onError": "continueRegularOutput",
        "notes": "Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets"
      },
      {
        "id": "code_1",
        "type": "n8n-nodes-base.code",
        "position": [
          850,
          300
        ],
        "parameters": {
          "jsCode": "// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"
        },
        "name": "Transform Data",
        "typeVersion": 2,
        "onError": "continueRegularOutput",
        "notes": "Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics"
      },
      {
        "id": "if_1",
        "type": "n8n-nodes-base.if",
        "position": [
          1150,
          300
        ],
        "parameters": {
          "conditions": {
            "conditions": [
              {
                "leftValue": "{{ $json.platform }}",
                "rightValue": "LinkedIn",
                "operator": {
                  "type": "string",
                  "operation": "equals"
                }
              }
            ]
          }
        },
        "name": "Filter Content",
        "typeVersion": 2.2,
        "onError": "continueRegularOutput",
        "notes": "Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets"
      },
      {
        "id": "googleSheets_1",
        "type": "n8n-nodes-base.googleSheets",
        "position": [
          1450,
          300
        ],
        "parameters": {
          "operation": "append",
          "range": "A:E",
          "columns": {
            "value": {
              "Platform": "={{ $json.platform }}",
              "URL": "={{ $json.url }}",
              "Title": "={{ $json.title }}",
              "Engagement": "={{ $json.engagement }}",
              "Type": "={{ $json.type }}"
            }
          }
        },
        "name": "Save to Google Sheets",
        "typeVersion": 4.6,
        "onError": "continueErrorOutput",
        "notes": "Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis"
      },
      {
        "id": "youTube_1",
        "type": "n8n-nodes-base.youTube",
        "position": [
          550,
          500
        ],
        "parameters": {
          "operation": "search",
          "part": [
            "snippet",
            "statistics"
          ],
          "q": "={{ $json.keyword }}",
          "order": "viewCount",
          "maxResults": 10
        },
        "name": "YouTube Search",
        "typeVersion": 2,
        "onError": "continueErrorOutput",
        "notes": "Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics"
      },
      {
        "id": "linkedIn_1",
        "type": "n8n-nodes-base.linkedIn",
        "position": [
          550,
          100
        ],
        "parameters": {
          "operation": "post",
          "person": ""
        },
        "name": "LinkedIn Posts",
        "typeVersion": 1,
        "onError": "continueErrorOutput",
        "notes": "This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead"
      }
    ],
    "connections": {
      "Webhook Trigger": {
        "main": [
          [
            {
              "node": "Research Agent",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Research Agent": {
        "main": [
          [
            {
              "node": "Transform Data",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Transform Data": {
        "main": [
          [
            {
              "node": "Filter Content",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Filter Content": {
        "main": [
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ],
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    },
    "settings": {
      "executionOrder": "v1",
      "saveDataSuccessExecution": "all",
      "saveDataErrorExecution": "all",
      "saveManualExecutions": true
    },
    "valid": true
  },
  "report": {
    "initial": {
      "workflow": {
        "errors": [
          {
            "node": "Webhook Trigger",
            "message": {
              "type": "missing_required",
              "property": "path",
              "message": "Required property 'Path' is missing",
              "fix": "Add path to your configuration"
            }
          },
          {
            "node": "Research Agent",
            "message": "Unknown node type: \"n8n-nodes-langchain.agent\". Node types must include the package prefix (e.g., \"n8n-nodes-base.webhook\", not \"webhook\" or \"nodes-base.webhook\")."
          },
          {
            "node": "Webhook Trigger",
            "message": "Node-level properties notes are in the wrong location. They must be at the node level, not inside parameters.",
            "details": {
              "fix": "Move these properties from node.parameters to the node level. Example:\n{\n  \"name\": \"Webhook Trigger\",\n  \"type\": \"n8n-nodes-base.webhook\",\n  \"parameters\": { /* operation-specific params */ },\n  \"onError\": \"continueErrorOutput\",  // ✅ Correct location\n  \"retryOnFail\": true,               // ✅ Correct location\n  \"executeOnce\": true,               // ✅ Correct location\n  \"disabled\": false,                 // ✅ Correct location\n  \"credentials\": { /* ... */ }       // ✅ Correct location\n}"
            }
          },
          {
            "node": "Research Agent",
            "message": "Node-level properties notes are in the wrong location. They must be at the node level, not inside parameters.",
            "details": {
              "fix": "Move these properties from node.parameters to the node level. Example:\n{\n  \"name\": \"Research Agent\",\n  \"type\": \"n8n-nodes-langchain.agent\",\n  \"parameters\": { /* operation-specific params */ },\n  \"onError\": \"continueErrorOutput\",  // ✅ Correct location\n  \"retryOnFail\": true,               // ✅ Correct location\n  \"executeOnce\": true,               // ✅ Correct location\n  \"disabled\": false,                 // ✅ Correct location\n  \"credentials\": { /* ... */ }       // ✅ Correct location\n}"
            }
          },
          {
            "node": "Transform Data",
            "message": "Node-level properties notes are in the wrong location. They must be at the node level, not inside parameters.",
            "details": {
              "fix": "Move these properties from node.parameters to the node level. Example:\n{\n  \"name\": \"Transform Data\",\n  \"type\": \"n8n-nodes-base.code\",\n  \"parameters\": { /* operation-specific params */ },\n  \"onError\": \"continueErrorOutput\",  // ✅ Correct location\n  \"retryOnFail\": true,               // ✅ Correct location\n  \"executeOnce\": true,               // ✅ Correct location\n  \"disabled\": false,                 // ✅ Correct location\n  \"credentials\": { /* ... */ }       // ✅ Correct location\n}"
            }
          },
          {
            "node": "Filter Content",
            "message": "Node-level properties notes are in the wrong location. They must be at the node level, not inside parameters.",
            "details": {
              "fix": "Move these properties from node.parameters to the node level. Example:\n{\n  \"name\": \"Filter Content\",\n  \"type\": \"n8n-nodes-base.if\",\n  \"parameters\": { /* operation-specific params */ },\n  \"onError\": \"continueErrorOutput\",  // ✅ Correct location\n  \"retryOnFail\": true,               // ✅ Correct location\n  \"executeOnce\": true,               // ✅ Correct location\n  \"disabled\": false,                 // ✅ Correct location\n  \"credentials\": { /* ... */ }       // ✅ Correct location\n}"
            }
          },
          {
            "node": "Save to Google Sheets",
            "message": "Node-level properties notes are in the wrong location. They must be at the node level, not inside parameters.",
            "details": {
              "fix": "Move these properties from node.parameters to the node level. Example:\n{\n  \"name\": \"Save to Google Sheets\",\n  \"type\": \"n8n-nodes-base.googleSheets\",\n  \"parameters\": { /* operation-specific params */ },\n  \"onError\": \"continueErrorOutput\",  // ✅ Correct location\n  \"retryOnFail\": true,               // ✅ Correct location\n  \"executeOnce\": true,               // ✅ Correct location\n  \"disabled\": false,                 // ✅ Correct location\n  \"credentials\": { /* ... */ }       // ✅ Correct location\n}"
            }
          },
          {
            "node": "YouTube Search",
            "message": "Node-level properties notes are in the wrong location. They must be at the node level, not inside parameters.",
            "details": {
              "fix": "Move these properties from node.parameters to the node level. Example:\n{\n  \"name\": \"YouTube Search\",\n  \"type\": \"n8n-nodes-base.youTube\",\n  \"parameters\": { /* operation-specific params */ },\n  \"onError\": \"continueErrorOutput\",  // ✅ Correct location\n  \"retryOnFail\": true,               // ✅ Correct location\n  \"executeOnce\": true,               // ✅ Correct location\n  \"disabled\": false,                 // ✅ Correct location\n  \"credentials\": { /* ... */ }       // ✅ Correct location\n}"
            }
          },
          {
            "node": "LinkedIn Posts",
            "message": "Node-level properties notes are in the wrong location. They must be at the node level, not inside parameters.",
            "details": {
              "fix": "Move these properties from node.parameters to the node level. Example:\n{\n  \"name\": \"LinkedIn Posts\",\n  \"type\": \"n8n-nodes-base.linkedIn\",\n  \"parameters\": { /* operation-specific params */ },\n  \"onError\": \"continueErrorOutput\",  // ✅ Correct location\n  \"retryOnFail\": true,               // ✅ Correct location\n  \"executeOnce\": true,               // ✅ Correct location\n  \"disabled\": false,                 // ✅ Correct location\n  \"credentials\": { /* ... */ }       // ✅ Correct location\n}"
            }
          },
          {
            "node": "workflow",
            "message": "Workflow validation failed: e.message.includes is not a function"
          },
          {
            "node": "Webhook Trigger",
            "message": "Outdated typeVersion: 1. Latest is 2",
            "type": "typeVersion",
            "severity": "error"
          },
          {
            "node": "Filter Content",
            "message": "Outdated typeVersion: 2. Latest is 2.2",
            "type": "typeVersion",
            "severity": "error"
          },
          {
            "node": "Save to Google Sheets",
            "message": "Outdated typeVersion: 4. Latest is 4.6",
            "type": "typeVersion",
            "severity": "error"
          }
        ],
        "warnings": [
          {
            "node": "YouTube Search",
            "message": "Node is not connected to any other nodes"
          },
          {
            "node": "LinkedIn Posts",
            "message": "Node is not connected to any other nodes"
          },
          {
            "node": "YouTube Search",
            "message": "Expression warning: q: Using $json but node might not have input data"
          },
          {
            "node": "workflow",
            "message": "Consider adding error handling to your workflow"
          },
          {
            "node": "Research Agent",
            "message": "AI Agent has no tools connected. Consider adding tools to enhance agent capabilities."
          }
        ],
        "valid": false,
        "statistics": {
          "totalNodes": 7,
          "enabledNodes": 7,
          "triggerNodes": 1,
          "validConnections": 5,
          "invalidConnections": 0,
          "expressionsValidated": 4,
          "errorCount": 10,
          "warningCount": 8
        }
      },
      "connections": {
        "errors": [],
        "warnings": []
      },
      "expressions": {
        "errors": [],
        "warnings": []
      }
    },
    "fixesApplied": [
      {
        "type": "entity-replacement",
        "attempt": 1,
        "timestamp": "2025-08-12T15:08:06.602Z",
        "description": "Replaced 7 nodes",
        "reasoning": [
          "Added required 'path' parameter to Webhook Trigger with value 'research-agent'",
          "Fixed node type for Research Agent from 'n8n-nodes-langchain.agent' to '@n8n/n8n-nodes-langchain.agent' with proper package prefix",
          "Moved all 'notes' properties from parameters to node level for all nodes",
          "Updated typeVersion for Webhook Trigger from 1 to 2 (latest)",
          "Updated typeVersion for Filter Content from 2 to 2.2 (latest)",
          "Updated typeVersion for Save to Google Sheets from 4 to 4.6 (latest)",
          "Added proper webhook configuration with httpMethod and responseMode"
        ],
        "entitiesFixed": {
          "nodes": [
            "webhook_1",
            "agent_1",
            "code_1",
            "if_1",
            "googleSheets_1",
            "youTube_1",
            "linkedIn_1"
          ],
          "connections": false
        }
      },
      {
        "type": "entity-replacement",
        "attempt": 2,
        "timestamp": "2025-08-12T15:08:09.015Z",
        "description": "Replaced 1 nodes",
        "reasoning": [
          "Updated typeVersion from 1 to 2 for the Research Agent node to match the latest version requirement"
        ],
        "entitiesFixed": {
          "nodes": [
            "agent_1"
          ],
          "connections": false
        }
      }
    ],
    "final": {
      "workflow": {
        "errors": [],
        "warnings": [
          {
            "node": "YouTube Search",
            "message": "Node is not connected to any other nodes"
          },
          {
            "node": "LinkedIn Posts",
            "message": "Node is not connected to any other nodes"
          },
          {
            "node": "YouTube Search",
            "message": "Expression warning: q: Using $json but node might not have input data"
          },
          {
            "node": "workflow",
            "message": "Consider adding error handling to your workflow"
          },
          {
            "node": "Research Agent",
            "message": "AI Agent has no tools connected. Consider adding tools to enhance agent capabilities."
          }
        ],
        "valid": true,
        "statistics": {
          "totalNodes": 7,
          "enabledNodes": 7,
          "triggerNodes": 1,
          "validConnections": 5,
          "invalidConnections": 0,
          "expressionsValidated": 4,
          "errorCount": 0,
          "warningCount": 5
        }
      },
      "connections": {
        "errors": [],
        "warnings": []
      },
      "expressions": {
        "errors": [],
        "warnings": []
      }
    },
    "attempts": 3
  }
}
```

**Transformations:**
- Validation Check
- Error Detection
- Automatic Fixes

### Session State Changes
**Changed from building phase:**

**Added:**
- `state`: {1 fields}

**Modified:**
- `state`: {11 fields} → {11 fields}

**Unchanged (2 fields):** sessionId, createdAt

## ========== DOCUMENTATION PHASE ==========

**Status:** ✅ Success
**Duration:** 662ms
**Memory Delta:** 1MB

### Logs
```
2025-08-12T15:08:10.005Z [INFO] [Orchestrator] Starting documentation phase
2025-08-12T15:08:10.005Z [DEBUG] [Claude] Sending request for documentation phase
2025-08-12T15:08:10.530Z [INFO] [Orchestrator] Added 6 sticky notes for documentation
2025-08-12T15:08:10.667Z [DEBUG] [Orchestrator] Session state updated
2025-08-12T15:08:10.667Z [INFO] [Orchestrator] Data flow captured: 3 transformations
```

### Data Flow
**Input:**
```json
{
  "validatedWorkflow": {
    "name": "Research Agent - LinkedIn & YouTube Scraper to Google Sheets",
    "nodes": [
      {
        "id": "webhook_1",
        "type": "n8n-nodes-base.webhook",
        "position": [
          470,
          300
        ],
        "parameters": {
          "path": "research-agent",
          "httpMethod": "POST",
          "responseMode": "onReceived"
        },
        "name": "Webhook Trigger",
        "typeVersion": 2,
        "onError": "stopWorkflow",
        "notes": "Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"
      },
      {
        "id": "agent_1",
        "type": "@n8n/n8n-nodes-langchain.agent",
        "position": [
          870,
          300
        ],
        "parameters": {
          "text": "{{ $json.keyword }}"
        },
        "name": "Research Agent",
        "typeVersion": 2,
        "onError": "continueRegularOutput",
        "notes": "Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets"
      },
      {
        "id": "code_1",
        "type": "n8n-nodes-base.code",
        "position": [
          1270,
          300
        ],
        "parameters": {
          "jsCode": "// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"
        },
        "name": "Transform Data",
        "typeVersion": 2,
        "onError": "continueRegularOutput",
        "notes": "Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics"
      },
      {
        "id": "if_1",
        "type": "n8n-nodes-base.if",
        "position": [
          1670,
          300
        ],
        "parameters": {
          "conditions": {
            "conditions": [
              {
                "leftValue": "{{ $json.platform }}",
                "rightValue": "LinkedIn",
                "operator": {
                  "type": "string",
                  "operation": "equals"
                }
              }
            ]
          }
        },
        "name": "Filter Content",
        "typeVersion": 2.2,
        "onError": "continueRegularOutput",
        "notes": "Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets"
      },
      {
        "id": "googleSheets_1",
        "type": "n8n-nodes-base.googleSheets",
        "position": [
          2070,
          300
        ],
        "parameters": {
          "operation": "append",
          "range": "A:E",
          "columns": {
            "value": {
              "Platform": "={{ $json.platform }}",
              "URL": "={{ $json.url }}",
              "Title": "={{ $json.title }}",
              "Engagement": "={{ $json.engagement }}",
              "Type": "={{ $json.type }}"
            }
          }
        },
        "name": "Save to Google Sheets",
        "typeVersion": 4.6,
        "onError": "continueErrorOutput",
        "notes": "Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis"
      },
      {
        "id": "youTube_1",
        "type": "n8n-nodes-base.youTube",
        "position": [
          550,
          500
        ],
        "parameters": {
          "operation": "search",
          "part": [
            "snippet",
            "statistics"
          ],
          "q": "={{ $json.keyword }}",
          "order": "viewCount",
          "maxResults": 10
        },
        "name": "YouTube Search",
        "typeVersion": 2,
        "onError": "continueErrorOutput",
        "notes": "Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics"
      },
      {
        "id": "linkedIn_1",
        "type": "n8n-nodes-base.linkedIn",
        "position": [
          550,
          100
        ],
        "parameters": {
          "operation": "post",
          "person": ""
        },
        "name": "LinkedIn Posts",
        "typeVersion": 1,
        "onError": "continueErrorOutput",
        "notes": "This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead"
      }
    ],
    "connections": {
      "Webhook Trigger": {
        "main": [
          [
            {
              "node": "Research Agent",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Research Agent": {
        "main": [
          [
            {
              "node": "Transform Data",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Transform Data": {
        "main": [
          [
            {
              "node": "Filter Content",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Filter Content": {
        "main": [
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ],
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    },
    "settings": {
      "executionOrder": "v1",
      "saveDataSuccessExecution": "all",
      "saveDataErrorExecution": "all",
      "saveManualExecutions": true
    },
    "valid": true
  }
}
```

**Output:**
```json
{
  "documentedWorkflow": {
    "name": "Research Agent - LinkedIn & YouTube Scraper to Google Sheets",
    "nodes": [
      {
        "id": "webhook_1",
        "type": "n8n-nodes-base.webhook",
        "position": [
          470,
          300
        ],
        "parameters": {
          "path": "research-agent",
          "httpMethod": "POST",
          "responseMode": "onReceived"
        },
        "name": "Webhook Trigger",
        "typeVersion": 2,
        "onError": "stopWorkflow",
        "notes": "Webhook trigger to initiate research agent workflow for scraping top performing LinkedIn posts and YouTube videos based on keyword input"
      },
      {
        "id": "agent_1",
        "type": "@n8n/n8n-nodes-langchain.agent",
        "position": [
          870,
          300
        ],
        "parameters": {
          "text": "{{ $json.keyword }}"
        },
        "name": "Research Agent",
        "typeVersion": 2,
        "onError": "continueRegularOutput",
        "notes": "Research agent that uses Apify to scrape top performing LinkedIn posts and YouTube videos based on a keyword, then outputs all links to Google Sheets"
      },
      {
        "id": "code_1",
        "type": "n8n-nodes-base.code",
        "position": [
          1270,
          300
        ],
        "parameters": {
          "jsCode": "// Transform agent output into structured data for Google Sheets\nconst items = [];\n\n// Process agent response\nif ($input.all().length > 0) {\n  const agentData = $input.all()[0].json;\n  \n  // Extract LinkedIn posts\n  if (agentData.linkedinPosts) {\n    agentData.linkedinPosts.forEach(post => {\n      items.push({\n        platform: 'LinkedIn',\n        url: post.url,\n        title: post.title || 'LinkedIn Post',\n        engagement: post.likes || 0,\n        type: 'Post'\n      });\n    });\n  }\n  \n  // Extract YouTube videos\n  if (agentData.youtubeVideos) {\n    agentData.youtubeVideos.forEach(video => {\n      items.push({\n        platform: 'YouTube',\n        url: video.url,\n        title: video.title || 'YouTube Video',\n        engagement: video.views || 0,\n        type: 'Video'\n      });\n    });\n  }\n}\n\nreturn items;"
        },
        "name": "Transform Data",
        "typeVersion": 2,
        "onError": "continueRegularOutput",
        "notes": "Transform scraped LinkedIn posts and YouTube videos data from Apify into a clean format for Google Sheets output with links and performance metrics"
      },
      {
        "id": "if_1",
        "type": "n8n-nodes-base.if",
        "position": [
          1670,
          300
        ],
        "parameters": {
          "conditions": {
            "conditions": [
              {
                "leftValue": "{{ $json.platform }}",
                "rightValue": "LinkedIn",
                "operator": {
                  "type": "string",
                  "operation": "equals"
                }
              }
            ]
          }
        },
        "name": "Filter Content",
        "typeVersion": 2.2,
        "onError": "continueRegularOutput",
        "notes": "Filter scraped content to separate LinkedIn posts from YouTube videos for proper routing to Google Sheets"
      },
      {
        "id": "googleSheets_1",
        "type": "n8n-nodes-base.googleSheets",
        "position": [
          2070,
          300
        ],
        "parameters": {
          "operation": "append",
          "range": "A:E",
          "columns": {
            "value": {
              "Platform": "={{ $json.platform }}",
              "URL": "={{ $json.url }}",
              "Title": "={{ $json.title }}",
              "Engagement": "={{ $json.engagement }}",
              "Type": "={{ $json.type }}"
            }
          }
        },
        "name": "Save to Google Sheets",
        "typeVersion": 4.6,
        "onError": "continueErrorOutput",
        "notes": "Append scraped LinkedIn posts and YouTube video links to Google Sheets for research analysis"
      },
      {
        "id": "youTube_1",
        "type": "n8n-nodes-base.youTube",
        "position": [
          550,
          500
        ],
        "parameters": {
          "operation": "search",
          "part": [
            "snippet",
            "statistics"
          ],
          "q": "={{ $json.keyword }}",
          "order": "viewCount",
          "maxResults": 10
        },
        "name": "YouTube Search",
        "typeVersion": 2,
        "onError": "continueErrorOutput",
        "notes": "Searches YouTube for top performing videos based on keyword input and retrieves video details with statistics"
      },
      {
        "id": "linkedIn_1",
        "type": "n8n-nodes-base.linkedIn",
        "position": [
          550,
          100
        ],
        "parameters": {
          "operation": "post",
          "person": ""
        },
        "name": "LinkedIn Posts",
        "typeVersion": 1,
        "onError": "continueErrorOutput",
        "notes": "This LinkedIn node is configured for creating posts, but it cannot scrape top performing posts - you'll need to use Apify's LinkedIn scraper instead"
      },
      {
        "id": "sticky_triggers_1755011290138",
        "name": "Triggers Documentation",
        "type": "n8n-nodes-base.stickyNote",
        "typeVersion": 1,
        "position": [
          390,
          -140
        ],
        "parameters": {
          "content": "## 📥 Triggers\nWebhook receives keyword input to initiate the research workflow for scraping top performing content.",
          "height": 780,
          "width": 310,
          "color": 5
        }
      },
      {
        "id": "sticky_inputs_1755011290138",
        "name": "Inputs Documentation",
        "type": "n8n-nodes-base.stickyNote",
        "typeVersion": 1,
        "position": [
          790,
          -140
        ],
        "parameters": {
          "content": "## 📊 Inputs\nAI research agent orchestrates the scraping of LinkedIn posts and YouTube videos using Apify based on the provided keyword.",
          "height": 780,
          "width": 310,
          "color": 5
        }
      },
      {
        "id": "sticky_transforms_1755011290138",
        "name": "Transform Documentation",
        "type": "n8n-nodes-base.stickyNote",
        "typeVersion": 1,
        "position": [
          1190,
          -140
        ],
        "parameters": {
          "content": "## ⚙️ Transform\nTransform and structure the scraped data from both platforms into a consistent format for Google Sheets storage.",
          "height": 780,
          "width": 310,
          "color": 5
        }
      },
      {
        "id": "sticky_decision_1755011290138",
        "name": "Decision Documentation",
        "type": "n8n-nodes-base.stickyNote",
        "typeVersion": 1,
        "position": [
          1590,
          -140
        ],
        "parameters": {
          "content": "## 🔀 Decision\nFilter and route content based on platform type to ensure proper categorization in the final output.",
          "height": 780,
          "width": 310,
          "color": 2
        }
      },
      {
        "id": "sticky_storage_1755011290138",
        "name": "Storage Documentation",
        "type": "n8n-nodes-base.stickyNote",
        "typeVersion": 1,
        "position": [
          1990,
          -140
        ],
        "parameters": {
          "content": "## 💾 Storage\nSave all scraped LinkedIn posts and YouTube video links with performance metrics to Google Sheets for analysis.",
          "height": 780,
          "width": 310,
          "color": 5
        }
      },
      {
        "id": "sticky_promo_1755011290139",
        "name": "Workflow Overview",
        "type": "n8n-nodes-base.stickyNote",
        "typeVersion": 1,
        "position": [
          50,
          -140
        ],
        "parameters": {
          "content": "## 🚀 Grow your AI business\n\nNeed help in implementing this workflow for your business? Join the Ghost Team community.\n\nThis workflow is made with 💚 by Ghost Team.",
          "height": 780,
          "width": 280,
          "color": 4
        }
      }
    ],
    "connections": {
      "Webhook Trigger": {
        "main": [
          [
            {
              "node": "Research Agent",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Research Agent": {
        "main": [
          [
            {
              "node": "Transform Data",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Transform Data": {
        "main": [
          [
            {
              "node": "Filter Content",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Filter Content": {
        "main": [
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ],
          [
            {
              "node": "Save to Google Sheets",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    },
    "settings": {
      "executionOrder": "v1",
      "saveDataSuccessExecution": "all",
      "saveDataErrorExecution": "all",
      "saveManualExecutions": true
    },
    "valid": true
  },
  "stickyNotesAdded": 6
}
```

**Transformations:**
- Documentation Generation
- Sticky Note Placement
- Workflow Finalization

### Session State Changes
**Changed from validation phase:**

**Modified:**
- `state`: {11 fields} → {11 fields}

**Unchanged (2 fields):** sessionId, createdAt

## Token Usage Report

## Summary

### Metrics
- **Total Nodes:** 13
- **Total Connections:** 4
- **Validation Attempts:** 3
- **Errors Fixed:** 2
- **Sticky Notes Added:** 6

### Scores
- **Performance Score:** 20/100
- **Quality Score:** 115/100
- **Completeness Score:** 100/100

### Optimization Suggestions
- Consider optimizing discovery phase (took 18103ms)
- Consider optimizing configuration phase (took 46618ms)
- Consider optimizing building phase (took 26046ms)
- Consider optimizing validation phase (took 18628ms)
- High validation attempts detected. Consider improving initial node configuration.
